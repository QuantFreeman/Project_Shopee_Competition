{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os, shutil, glob, os.path\n",
    "from PIL import Image as pil_image\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD \n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - ETA:  - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#image.LOAD_TRUNCATED_IMAGES = True \n",
    "model = EfficientNetB0(weights='imagenet',include_top=False,pooling=\"avg\",input_shape=None)\n",
    "imdir = \"data/train_images/\"\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "y = df.label_group\n",
    "number_clusters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add path to df\n",
    "df['path'] = imdir + df.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Status: 34249 / 34250\r"
     ]
    }
   ],
   "source": [
    "featurelist = []\n",
    "for i, imagepath in enumerate(df.path):\n",
    "    print(\"    Status: %s / %s\" %(i, len(df.path)), end=\"\\r\")\n",
    "    img = image.load_img(imagepath, target_size=(224, 224))\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "    features = np.array(model.predict(img_data))\n",
    "    featurelist.append(features.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file into pickle\n",
    "with open(\"featurelist_effnet.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(featurelist, fp)\n",
    " \n",
    "# with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "#     b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_predictions(df, x,y ,threshold = 0.4):\n",
    "    if len(df) > 3:\n",
    "        KNN = 50\n",
    "    else : \n",
    "        KNN = 3\n",
    "    #model = NearestNeighbors(n_neighbors = KNN)\n",
    "    model = KNeighborsClassifier(n_neighbors= KNN, metric='cosine')\n",
    "    model.fit(x,y)\n",
    "    distances, indices = model.kneighbors(x)\n",
    "    predictions = []\n",
    "    for k in range(x.shape[0]):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = df['posting_id'].iloc[ids].values\n",
    "        predictions.append(posting_ids)\n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return predictions\n",
    "\n",
    "\n",
    "#text preds\n",
    "\n",
    "def get_text_predictions(df, max_features = 25_000):\n",
    "    stopwords_list = stopwords.words('english') + stopwords.words('indonesian')\n",
    "    model = TfidfVectorizer(stop_words = stopwords_list, binary = True, max_features = max_features)\n",
    "    \n",
    "    text_embeddings = model.fit_transform(df['title']).toarray()\n",
    "    preds = []\n",
    "    # magic number\n",
    "    CHUNK = 1024*4\n",
    "    print('Finding similar titles...')\n",
    "    # discern # of chunks\n",
    "    CTS = len(df)//CHUNK\n",
    "    if len(df)%CHUNK!=0: CTS += 1\n",
    "    for j in range( CTS ):\n",
    "        # start pos for chunk idx\n",
    "        a = j*CHUNK\n",
    "        # end pos for chunk idx\n",
    "        b = (j+1)*CHUNK\n",
    "        # change to end of input if necessary\n",
    "        b = min(b,len(df))\n",
    "        print('chunk',a,'to',b)\n",
    "        # COSINE SIMILARITY DISTANCE\n",
    "        # matrix multiply(text emb & transposed chunk) then transpose\n",
    "        cts = np.matmul( text_embeddings, text_embeddings[a:b].T ).T\n",
    "        for k in range(b-a):\n",
    "            # find where cosine sim > 0.7\n",
    "            IDX = np.where(cts[k,]>0.7)[0]\n",
    "            # save to o\n",
    "            o = df.iloc[IDX].posting_id.values\n",
    "            # append to preds\n",
    "            preds.append(o)\n",
    "    # delet model/text emb\n",
    "    del model,text_embeddings\n",
    "    # garb collect\n",
    "    gc.collect()\n",
    "    return preds\n",
    "\n",
    "# clean text from noise\n",
    "def clean_text(text):\n",
    "    # filter to allow only alphabets\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    \n",
    "    # remove Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "       \n",
    "    return text\n",
    "\n",
    "#train['clean_text'] = train.tweet.apply(clean_text)\n",
    "\n",
    "#combine prediction into 1 row\n",
    "\n",
    "def combine_predictions(row):\n",
    "    x = np.concatenate([row['img_prediction'],     row['text_predictions']])\n",
    "    return ' '.join( np.unique(x) )\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(featurelist, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get image prediction \n",
    "preds = get_image_predictions(df, x, y, threshold = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list every label targets\n",
    "tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "df['targets'] = df['label_group'].map(tmp)\n",
    "df['targets'] = df['targets'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuri\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar titles...\n",
      "chunk 0 to 4096\n",
      "chunk 4096 to 8192\n",
      "chunk 8192 to 12288\n",
      "chunk 12288 to 16384\n",
      "chunk 16384 to 20480\n",
      "chunk 20480 to 24576\n",
      "chunk 24576 to 28672\n",
      "chunk 28672 to 32768\n",
      "chunk 32768 to 34250\n"
     ]
    }
   ],
   "source": [
    "text_predictions = get_text_predictions(df, max_features = 25_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_predictions'] = text_predictions\n",
    "df['img_prediction'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['matches'] = df.apply(combine_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>path</th>\n",
       "      <th>targets</th>\n",
       "      <th>text_predictions</th>\n",
       "      <th>img_prediction</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>data/train_images/0000a68812bc7e98c42888dfb1c0...</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>[train_129225211, train_197296533]</td>\n",
       "      <td>train_129225211 train_197296533 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>data/train_images/00039780dfc94d01db8676fe789e...</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "      <td>[train_3386243561]</td>\n",
       "      <td>[train_3386243561, train_3423213080, train_212...</td>\n",
       "      <td>train_1387702006 train_1553039102 train_181696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>data/train_images/000a190fdd715a2a36faed16e2c6...</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>[train_2288590299]</td>\n",
       "      <td>[train_2288590299, train_2723454438, train_326...</td>\n",
       "      <td>train_2288590299 train_2723454438 train_326726...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>data/train_images/00117e4fc239b1b641ff08340b42...</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>[train_2406599165, train_3576714541, train_150...</td>\n",
       "      <td>[train_2406599165, train_1593362411, train_256...</td>\n",
       "      <td>train_1002655969 train_1029583218 train_106133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>data/train_images/00136d1cf4edede0203f32f05f66...</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "      <td>[train_3369186413]</td>\n",
       "      <td>[train_3369186413, train_921438619, train_2194...</td>\n",
       "      <td>train_1043687807 train_1093166739 train_115421...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_2464356923</td>\n",
       "      <td>0013e7355ffc5ff8fb1ccad3e42d92fe.jpg</td>\n",
       "      <td>bbd097a7870f4a50</td>\n",
       "      <td>CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa...</td>\n",
       "      <td>2660605217</td>\n",
       "      <td>data/train_images/0013e7355ffc5ff8fb1ccad3e42d...</td>\n",
       "      <td>train_2464356923 train_2753295474 train_305884580</td>\n",
       "      <td>[train_2464356923]</td>\n",
       "      <td>[train_2464356923, train_2753295474, train_305...</td>\n",
       "      <td>train_2464356923 train_2753295474 train_305884580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_1802986387</td>\n",
       "      <td>00144a49c56599d45354a1c28104c039.jpg</td>\n",
       "      <td>f815c9bb833ab4c8</td>\n",
       "      <td>Jubah anak size 1-12 thn</td>\n",
       "      <td>1835033137</td>\n",
       "      <td>data/train_images/00144a49c56599d45354a1c28104...</td>\n",
       "      <td>train_1802986387 train_1396161074 train_713073...</td>\n",
       "      <td>[train_1802986387, train_1396161074, train_249...</td>\n",
       "      <td>[train_1802986387, train_944158112, train_1513...</td>\n",
       "      <td>train_1385709310 train_1396161074 train_151399...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_1806152124</td>\n",
       "      <td>0014f61389cbaa687a58e38a97b6383d.jpg</td>\n",
       "      <td>eea7e1c0c04da33d</td>\n",
       "      <td>KULOT PLISKET SALUR /CANDY PLISKET /WISH KULOT...</td>\n",
       "      <td>1565741687</td>\n",
       "      <td>data/train_images/0014f61389cbaa687a58e38a97b6...</td>\n",
       "      <td>train_1806152124 train_3227306976</td>\n",
       "      <td>[train_1806152124]</td>\n",
       "      <td>[train_1806152124, train_719431433, train_3560...</td>\n",
       "      <td>train_1064230135 train_1165410768 train_132369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_86570404</td>\n",
       "      <td>0019a3c6755a194cb2e2c12bfc63972e.jpg</td>\n",
       "      <td>ea9af4f483249972</td>\n",
       "      <td>[LOGU] Tempelan kulkas magnet angka, tempelan ...</td>\n",
       "      <td>2359912463</td>\n",
       "      <td>data/train_images/0019a3c6755a194cb2e2c12bfc63...</td>\n",
       "      <td>train_86570404 train_2837452969 train_77364776</td>\n",
       "      <td>[train_86570404]</td>\n",
       "      <td>[train_86570404, train_2837452969, train_77364...</td>\n",
       "      <td>train_115157077 train_2264584728 train_2269068...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_831680791</td>\n",
       "      <td>001be52b2beec40ddc1d2d7fc7a68f08.jpg</td>\n",
       "      <td>e1ce953d1a70618f</td>\n",
       "      <td>BIG SALE SEPATU PANTOFEL KULIT KEREN KERJA KAN...</td>\n",
       "      <td>2630990665</td>\n",
       "      <td>data/train_images/001be52b2beec40ddc1d2d7fc7a6...</td>\n",
       "      <td>train_831680791 train_3031035861</td>\n",
       "      <td>[train_831680791]</td>\n",
       "      <td>[train_831680791, train_3031035861, train_1480...</td>\n",
       "      <td>train_1035429011 train_1189949692 train_122084...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "5  train_2464356923  0013e7355ffc5ff8fb1ccad3e42d92fe.jpg  bbd097a7870f4a50   \n",
       "6  train_1802986387  00144a49c56599d45354a1c28104c039.jpg  f815c9bb833ab4c8   \n",
       "7  train_1806152124  0014f61389cbaa687a58e38a97b6383d.jpg  eea7e1c0c04da33d   \n",
       "8    train_86570404  0019a3c6755a194cb2e2c12bfc63972e.jpg  ea9af4f483249972   \n",
       "9   train_831680791  001be52b2beec40ddc1d2d7fc7a68f08.jpg  e1ce953d1a70618f   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "5  CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa...   2660605217   \n",
       "6                           Jubah anak size 1-12 thn   1835033137   \n",
       "7  KULOT PLISKET SALUR /CANDY PLISKET /WISH KULOT...   1565741687   \n",
       "8  [LOGU] Tempelan kulkas magnet angka, tempelan ...   2359912463   \n",
       "9  BIG SALE SEPATU PANTOFEL KULIT KEREN KERJA KAN...   2630990665   \n",
       "\n",
       "                                                path  \\\n",
       "0  data/train_images/0000a68812bc7e98c42888dfb1c0...   \n",
       "1  data/train_images/00039780dfc94d01db8676fe789e...   \n",
       "2  data/train_images/000a190fdd715a2a36faed16e2c6...   \n",
       "3  data/train_images/00117e4fc239b1b641ff08340b42...   \n",
       "4  data/train_images/00136d1cf4edede0203f32f05f66...   \n",
       "5  data/train_images/0013e7355ffc5ff8fb1ccad3e42d...   \n",
       "6  data/train_images/00144a49c56599d45354a1c28104...   \n",
       "7  data/train_images/0014f61389cbaa687a58e38a97b6...   \n",
       "8  data/train_images/0019a3c6755a194cb2e2c12bfc63...   \n",
       "9  data/train_images/001be52b2beec40ddc1d2d7fc7a6...   \n",
       "\n",
       "                                             targets  \\\n",
       "0                   train_129225211 train_2278313361   \n",
       "1                  train_3386243561 train_3423213080   \n",
       "2                  train_2288590299 train_3803689425   \n",
       "3                  train_2406599165 train_3342059966   \n",
       "4                   train_3369186413 train_921438619   \n",
       "5  train_2464356923 train_2753295474 train_305884580   \n",
       "6  train_1802986387 train_1396161074 train_713073...   \n",
       "7                  train_1806152124 train_3227306976   \n",
       "8     train_86570404 train_2837452969 train_77364776   \n",
       "9                   train_831680791 train_3031035861   \n",
       "\n",
       "                                    text_predictions  \\\n",
       "0                [train_129225211, train_2278313361]   \n",
       "1                                 [train_3386243561]   \n",
       "2                                 [train_2288590299]   \n",
       "3  [train_2406599165, train_3576714541, train_150...   \n",
       "4                                 [train_3369186413]   \n",
       "5                                 [train_2464356923]   \n",
       "6  [train_1802986387, train_1396161074, train_249...   \n",
       "7                                 [train_1806152124]   \n",
       "8                                   [train_86570404]   \n",
       "9                                  [train_831680791]   \n",
       "\n",
       "                                      img_prediction  \\\n",
       "0                 [train_129225211, train_197296533]   \n",
       "1  [train_3386243561, train_3423213080, train_212...   \n",
       "2  [train_2288590299, train_2723454438, train_326...   \n",
       "3  [train_2406599165, train_1593362411, train_256...   \n",
       "4  [train_3369186413, train_921438619, train_2194...   \n",
       "5  [train_2464356923, train_2753295474, train_305...   \n",
       "6  [train_1802986387, train_944158112, train_1513...   \n",
       "7  [train_1806152124, train_719431433, train_3560...   \n",
       "8  [train_86570404, train_2837452969, train_77364...   \n",
       "9  [train_831680791, train_3031035861, train_1480...   \n",
       "\n",
       "                                             matches  \n",
       "0   train_129225211 train_197296533 train_2278313361  \n",
       "1  train_1387702006 train_1553039102 train_181696...  \n",
       "2  train_2288590299 train_2723454438 train_326726...  \n",
       "3  train_1002655969 train_1029583218 train_106133...  \n",
       "4  train_1043687807 train_1093166739 train_115421...  \n",
       "5  train_2464356923 train_2753295474 train_305884580  \n",
       "6  train_1385709310 train_1396161074 train_151399...  \n",
       "7  train_1064230135 train_1165410768 train_132369...  \n",
       "8  train_115157077 train_2264584728 train_2269068...  \n",
       "9  train_1035429011 train_1189949692 train_122084...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(df.targets, df.matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7374797619328904"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x[0].split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_img = f2_score(df.targets, df.img_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46016187277377063"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_img.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7374797619328904"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
